import re
import json
import random
from collections import defaultdict
from typing import Dict, List, DefaultDict, Generator


def tokenize(text: str) -> List[str]:
    """–¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–∞, –ø—É—Å—Ç—ã–µ —Å—Ç—Ä–æ–∫–∏ –æ—Ç–±—Ä–∞—Å—ã–≤–∞—é—Ç—Å—è"""
    punctuation = "\\" + "\\".join(list("[](){}!?.,:;'\"\\/*&^%$_+-‚Äì‚Äî=<>@|~"))
    words = "[a-zA-Z–∞-—è–ê-–Ø—ë–Å]+"
    compounds = f"{words}-{words}"
    scr = "\\.{3}"

    tokenize_re = re.compile(f"({scr}|{compounds}|{words}|[{punctuation}])")
    newlines_re = re.compile(r"\n\s*")
    paragraphed = newlines_re.sub(NEWLINE_SYMBOL, text)

    return [token for token in tokenize_re.split(paragraphed) if token]


def slice_corpus(corpus: list, smpl_size: int) -> List[str]:
    """–ù–∞—Ä–µ–∑–∞–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ –Ω–∞ —Å–ø–∏—Å–∫–∏ –∑–∞–¥–∞–Ω–Ω–æ–≥–æ —Ä–∞–∑–º–µ—Ä–∞ smpl_size —Å —É—á–µ—Ç–æ–º –ø—Ä–æ–±–µ–ª–æ–≤"""
    samples = (corpus[i: i + smpl_size] for i, _ in enumerate(corpus))
    return [s for s in samples if len(s) == smpl_size]


def collect_transitions(samples: list) -> DefaultDict[str, str]:
    """–í—ã–¥–µ–ª–∏–º –∫–ª—é—á–∏ –∏ –ø–µ—Ä–µ—Ö–æ–¥—ã –ø–æ —Å—ç–º–ø–ª–∞–º –∏ —Å–æ–±–µ—Ä—ë–º –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –∫–ª—é—á–∞ —Å–ø–∏—Å–æ–∫ –≤—Å–µ—Ö –≤—Å—Ç—Ä–µ—Ç–∏–≤—à–∏—Ö—Å—è –ø–µ—Ä–µ—Ö–æ–¥–æ–≤:"""
    transitions = defaultdict(list)
    # –†–∞–∑–±–∏–≤–∞–µ–º —Å–µ–º–ø–ª –Ω–∞ —Ç–æ–∫–µ–Ω—ã –¥–ª—è –∫–ª—é—á–∞ –∏ —Ç–æ–∫–µ–Ω-–ø–µ—Ä–µ—Ö–æ–¥.
    for sample in samples:
        # –ò–∑ –ø–µ—Ä–≤—ã—Ö (n - 1) —Ç–æ–∫–µ–Ω–æ–≤ —Å–æ—Å—Ç–∞–≤–ª—è–µ–º –∫–ª—é—á,
        # –ø–æ –∫–æ—Ç–æ—Ä–æ–º—É –±—É–¥–µ–º –∏—Å–∫–∞—Ç—å —Å–ø–∏—Å–æ–∫ –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª—å–Ω—ã—Ö –ø–µ—Ä–µ—Ö–æ–¥–æ–≤:
        state = "".join(sample[0:-1])
        # –ü–æ—Å–ª–µ–¥–Ω–∏–π —Ç–æ–∫–µ–Ω ‚Äî –ø–µ—Ä–µ—Ö–æ–¥:
        nxt = sample[-1]
        # –î–∞–ª—å—à–µ –∏—â–µ–º —Å–ø–∏—Å–æ–∫, –µ—Å–ª–∏ –Ω–µ –Ω–∞—à–ª–∏ - —Å–æ–∑–¥–∞–µ–º,
        # –¥–æ–±–∞–≤–ª—è–µ–º –≤ –Ω–µ–≥–æ –Ω–æ–≤—ã–π —Ç–æ–∫–µ–Ω-–ø–µ—Ä–µ—Ö–æ–¥:
        transitions[state].append(nxt)
    return transitions


def predict_next_word(chain: List[str], transitions: DefaultDict[str, str], smpl_size: int) -> str:
    """"–ü–æ–∏—Å–∫ –Ω–æ–≤–æ–≥–æ —Ç–æ–∫–µ–Ω–∞ –ø—É—Ç–µ–º –ø–æ–∏—Å–∫–∞ –ø–æ –ø–æ—Å–ª–µ–¥–Ω–µ–º—É —Ç–æ–∫–µ–Ω—É –≤ –º–∞—Ç—Ä–∏—Ü–µ –≤–æ–∑–º–æ–∂–Ω—ã—Ö –ø–µ—Ä–µ—Ö–æ–¥–æ–≤"""
    last_state = "".join(chain[-(smpl_size - 1):])
    next_words = transitions[last_state]
    return random.choice(next_words) if next_words else ""


def create_chain(start_text: str, transitions: defaultdict) -> List[str]:
    """"–ò–Ω–∏—Ü–∏–∞—Ü–∏—è —Ü–µ–ø–∏ —Ç–æ–∫–µ–Ω–æ–≤"""
    head = start_text or random.choice(list(transitions.keys()))
    return tokenize(head)


def generate_chain(start_text: str, transitions: DefaultDict[str, str], smpl_size: int) -> Generator:
    """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Ü–µ–ø–∏ —Ç–æ–∫–µ–Ω–æ–≤"""
    chain = create_chain(start_text, transitions)
    while True:
        state = predict_next_word(chain, transitions, smpl_size)
        yield state

        if state:
            chain.append(state)
        else:
            chain = chain[:-1]


def tokens_to_text(tokens: List[str]) -> str:
    """–°–∫–ª–µ–∏–≤–∞–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –≤ —Ç–µ–∫—Å—Ç"""
    paragraph_char = '\n\n'
    return "".join(tokens).replace(NEWLINE_SYMBOL, paragraph_char)


def get_data_from_json_file() -> Dict[str, str]:
    """"–ü–∞—Ä—Å–∏–º json-—Ñ–∞–π–ª FILENAME —Å –≤—ã–≥—Ä—É–∑–∫–æ–π –∏–∑ –∫–∞–Ω–∞–ª–∞ telegram"""
    with open(FILENAME, encoding='utf-8') as f:
        templates = json.load(f)

    messages = {section['from']: '' for section in templates['messages'] if section.get('from')}
    for section in templates['messages']:
        if (section.get('from') is not None
                and isinstance(section['text'], str)):
            messages[section['from']] += section['text'] + ' '

    return messages


def get_sample_size(len_text_message: str) -> int:
    """"–û–ø—Ä–µ–¥–µ–ª—è–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å—ç–º–ø–ª–æ–≤ –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç –¥–ª–∏–Ω—ã —Å–ª–æ–≤–∞—Ä—è –∞–≤—Ç–æ—Ä–∞"""
    if len_text_message > MAX_COUNT:
        return MAX_SAMPLE
    elif MIN_COUNT <= len_text_message <= MAX_COUNT:
        return int(len_text_message // MIN_COUNT)
    else:
        return MIN_SAMPLE


def generate(source: str, sample_size: int = 2, start: str = '') -> str:
    """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Ç–æ–∫–µ–Ω–æ–≤"""
    corpus = tokenize(source)
    samples = slice_corpus(corpus, sample_size)
    transitions = collect_transitions(samples)
    generator = generate_chain(start, transitions, sample_size)
    generated_tokens = [next(generator) for _ in range(WORDS_COUNT)]

    return tokens_to_text(generated_tokens)


def main() -> None:
    messages = get_data_from_json_file()
    for key, text_message in messages.items():
        len_text_message = len(text_message)
        print(f'*** –û—Ç–≤–µ—á–∞–µ—Ç {key} (—á–∏—Å–ª–æ —Å–∏–º–≤–æ–ª–æ–≤ –≤ –∫–æ—Ä–ø—É—Å–µ: {len_text_message}) ***')
        sample_size = get_sample_size(len_text_message)
        print(generate(text_message, sample_size), '', sep='\n')

        
if __name__ == '__main__':
    # –æ–±—â–∏–µ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏
    NEWLINE_SYMBOL = "¬ß"  # –∑–Ω–∞–∫ –ø–∞—Ä–∞–≥—Ä–∞—Ñ–∞ –≤ —Ç–µ–∫—Å—Ç–µ
    FILENAME = 'input.json'  # –∏–º—è —Ñ–∞–π–ª–∞ —Å –¥–∞–Ω–Ω—ã–º–∏
    FROM = 'lvnvl üòê'  # –∏–º—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è, –ø–æ —Å–æ–æ–±—â–µ–Ω–∏—è–º –∫–æ—Ç–æ—Ä–æ–≥–æ —Ñ–æ—Ä–º–∏—Ä—É–µ—Ç—Å—è –æ—Ç–≤–µ—Ç

    # –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ –º–æ–¥–µ–ª–∏
    MAX_COUNT = 90000  # –º–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–π –ø—Ä–µ–¥–µ–ª –¥–ª—è –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è —Ä–∞–∑–º–µ—Ä–∞ sample
    MIN_COUNT = 10000  # –º–∏–Ω–∏–º–∞–ª—å–Ω—ã–π –ø—Ä–µ–¥–µ–ª –¥–ª—è –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è —Ä–∞–∑–º–µ—Ä–∞ sample
    MAX_SAMPLE = 8  # –º–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–π —Ä–∞–∑–º–µ—Ä sample
    MIN_SAMPLE = 5  # –º–∏–Ω–∏–º–∞–ª—å–Ω—ã–π —Ä–∞–∑–º–µ—Ä sample
    WORDS_COUNT = 100  # –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–ª–æ–≤ –≤ —Ñ–æ—Ä–º–∏—Ä—É–µ–º–æ–º —Å–æ–æ–±—â–µ–Ω–∏–∏

    main()
